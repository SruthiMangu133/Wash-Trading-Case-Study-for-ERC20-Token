# -*- coding: utf-8 -*-
"""streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-KTP0AqB-kf2_Wiz16D_ji9k587Dwpv
"""

!pip install streamlit pyngrok pandas numpy matplotlib seaborn networkx statsmodels scikit-learn tensorflow

!streamlit run app.py &>/dev/null &

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

def load_data(file):
    df = pd.read_csv(file)
    return df

def create_network_features(df):
    G = nx.from_pandas_edgelist(df, source='from_address', target='to_address', edge_attr='quantity', create_using=nx.DiGraph())
    df['Degree Centrality'] = df['from_address'].map(nx.degree_centrality(G))
    df['Betweenness Centrality'] = df['from_address'].map(nx.betweenness_centrality(G))
    df['Closeness Centrality'] = df['from_address'].map(nx.closeness_centrality(G))
    return df

def preprocess_data(df):
    df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')
    df['quantity'] = df['quantity'].astype(str).str.replace(',', '').astype(float)
    df = create_network_features(df)

    features = ['quantity', 'Degree Centrality', 'Betweenness Centrality', 'Closeness Centrality']
    scaler = StandardScaler()
    df[features] = scaler.fit_transform(df[features])

    return df

def detect_anomalies(df, model_name):
    features = df[['quantity', 'Degree Centrality', 'Betweenness Centrality', 'Closeness Centrality']].dropna()

    if model_name == "Isolation Forest":
        model = IsolationForest(contamination=0.05, random_state=42)
        df['anomaly'] = model.fit_predict(features)

    elif model_name == "Local Outlier Factor":
        model = LocalOutlierFactor(n_neighbors=20, contamination=0.05, novelty=False)
        df['anomaly'] = model.fit_predict(features)

    elif model_name == "One-Class SVM":
        model = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)
        df['anomaly'] = model.fit_predict(features)

    elif model_name == "LSTM":
        X = np.array(features).reshape((features.shape[0], features.shape[1], 1))
        model = Sequential([
            LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.fit(X, np.zeros(X.shape[0]), epochs=10, batch_size=32, verbose=0)

        predictions = model.predict(X)
        df['anomaly'] = (predictions < 0.5).astype(int)

    elif model_name == "ARIMA":
        df['anomaly'] = 0  # Default all to normal
        for index in range(len(df)):
            try:
                model = ARIMA(df['quantity'][:index+1], order=(5,1,0))
                fitted = model.fit()
                prediction = fitted.forecast(steps=1)
                df.loc[df.index[index], 'anomaly'] = 1 if abs(df['quantity'][index] - prediction) > 2 * df['quantity'].std() else 0
            except:
                continue

    elif model_name == "Ensemble":
        models = [
            IsolationForest(contamination=0.05, random_state=42),
            LocalOutlierFactor(n_neighbors=20, contamination=0.05, novelty=False),
            OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)
        ]
        predictions = np.zeros(len(features))
        for model in models:
            pred = model.fit_predict(features) if not isinstance(model, LocalOutlierFactor) else model.fit_predict(features)
            predictions += (pred == -1)
        df['anomaly'] = (predictions > 1).astype(int)

    df['anomaly'] = df['anomaly'].apply(lambda x: 1 if x == -1 else 0)
    return df

def main():
    st.title("ERC20 Transaction Anomaly Detection")
    uploaded_file = st.file_uploader("Upload ERC20 Transactions CSV", type=["csv"])
    model_choice = st.selectbox("Select Anomaly Detection Model", [
        "Isolation Forest", "Local Outlier Factor", "One-Class SVM", "LSTM", "ARIMA", "Ensemble"
    ])

    if "processed_df" not in st.session_state:
        st.session_state.processed_df = None
    if "anomaly_detected" not in st.session_state:
        st.session_state.anomaly_detected = False

    process_data = st.button("Process Data")
    if uploaded_file is not None and process_data:
        df = load_data(uploaded_file)
        st.session_state.processed_df = preprocess_data(df)
        st.session_state.anomaly_detected = False

    if st.session_state.processed_df is not None:
        detect_anomalies_button = st.button("Detect Anomalies")
        if detect_anomalies_button:
            st.session_state.processed_df = detect_anomalies(st.session_state.processed_df, model_choice)
            st.session_state.anomaly_detected = True

    if st.session_state.anomaly_detected:
        st.write("### Anomaly Detection Results")
        st.write(st.session_state.processed_df.head())

        plt.figure(figsize=(12, 6))
        sns.scatterplot(x=st.session_state.processed_df['date_time'], y=st.session_state.processed_df['quantity'], hue=st.session_state.processed_df['anomaly'], palette={1: 'red', 0: 'blue'})
        plt.title("Anomalies in Transactions")
        st.pyplot(plt)

        st.write("Download Processed Data")
        csv = st.session_state.processed_df.to_csv(index=False).encode('utf-8')
        st.download_button("Download CSV", data=csv, file_name="processed_transactions.csv", mime="text/csv")

if __name__ == "__main__":
    main()

!ngrok authtoken 2sOQmZbz0EhOBK60YAz17pRg3cV_7VrAQmNtf8RjCzPa6ztxJ

import os
import time
from pyngrok import ngrok

# Stop any existing Streamlit and ngrok processes
os.system("pkill -f streamlit")
os.system("pkill -f ngrok")

# Wait for processes to stop
time.sleep(2)

# Run the Streamlit app in the background
os.system('streamlit run app.py &')

# Wait for Streamlit to fully start
time.sleep(5)

# Open an ngrok tunnel to the Streamlit app on port 8501
public_url = ngrok.connect(8501)

# Print the URL to access the app
print("âœ… Streamlit app is live at:", public_url)